tensor([[ 1.0802e-02, -2.0381e-02, -1.0256e-01, -1.0207e-01,  9.7954e-03,
          4.8736e-02, -2.6985e-03, -5.5921e-02, -2.6604e-02,  6.8624e-02],
        [ 2.2883e-02,  4.6792e-03, -5.9218e-02, -2.4972e-02,  1.9629e-02,
          6.4607e-02, -5.8696e-02, -9.3555e-03,  2.3386e-03,  4.1580e-02],
        [-1.8079e-02, -1.9579e-02, -1.2329e-01, -7.1293e-02,  1.0452e-02,
          4.8156e-02, -3.6102e-02, -1.5397e-03,  2.8695e-03,  4.2025e-02],
        [-1.2806e-03, -1.7109e-02, -1.0494e-01, -5.9992e-02,  1.2770e-02,
          2.8107e-02, -4.4824e-02, -1.8101e-02, -2.8999e-03,  2.0773e-02],
        [ 2.8267e-02, -3.7847e-02, -8.6563e-02, -6.3748e-02,  2.5272e-02,
          3.6569e-02, -5.1479e-02, -4.4456e-02,  8.9298e-03,  4.9760e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [-5.8719e-03, -1.8455e-02, -1.0071e-01, -8.3821e-02,  1.2615e-03,
          4.8175e-02, -4.2655e-02, -4.3430e-02,  1.4499e-04,  3.7134e-02],
        [ 2.2060e-02, -3.1583e-02, -7.5225e-02, -5.1045e-02,  4.1165e-02,
          5.6417e-02, -4.0676e-02, -3.5867e-04,  8.0289e-03,  3.1257e-02],
        [ 1.3864e-02, -2.7852e-02, -8.4836e-02, -6.8402e-02,  3.0712e-02,
          5.0601e-02, -5.8443e-02, -6.9371e-03,  6.3078e-06,  3.2580e-02],
        [ 2.6802e-02, -3.9943e-02, -8.8605e-02, -5.7006e-02,  2.9432e-02,
          2.3662e-02, -2.8074e-02, -4.9268e-03,  4.7915e-03,  3.8245e-02],
        [-8.7160e-03, -1.0013e-01, -8.2872e-02, -4.2885e-02, -2.5808e-02,
          7.2205e-02, -1.8117e-02, -4.9148e-02, -5.7276e-02,  1.8719e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 8.9416e-03, -1.3459e-02, -1.1247e-01, -7.9509e-02,  4.9843e-02,
          1.4532e-02, -4.5723e-02, -2.4865e-02,  1.9165e-02,  2.0622e-02],
        [ 2.1321e-02, -2.2961e-02, -7.0892e-02, -2.5431e-02,  1.8812e-02,
          3.2573e-02, -8.9732e-02,  1.2430e-02,  1.6453e-02,  7.6123e-02],
        [ 7.0213e-02, -4.7491e-02, -7.9926e-02, -6.3570e-02,  2.8231e-02,
          3.0551e-02, -6.6498e-03, -5.5241e-02, -3.8193e-02,  4.1540e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 3.0635e-02, -8.8393e-03, -7.6141e-02, -2.2873e-02,  1.3132e-02,
          4.3151e-02, -5.0180e-02, -2.7549e-02,  1.8379e-02,  4.6593e-02],
        [ 4.5861e-02, -6.7058e-02, -9.1897e-02, -8.7046e-02,  3.9962e-02,
          5.5540e-02, -9.6556e-03, -5.0355e-02, -6.6946e-03,  4.0281e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 2.3339e-02, -9.0517e-02, -1.1418e-01, -5.1170e-02, -1.5575e-02,
          6.9639e-02, -4.4630e-02, -1.7297e-02, -4.5397e-02,  8.2140e-03],
        [ 8.7718e-03, -1.6241e-02, -9.3373e-02, -7.8076e-02,  1.6844e-02,
          3.1866e-02, -2.0079e-02, -3.7235e-02, -1.5514e-02,  6.4473e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 3.5920e-02, -1.3360e-02, -8.0571e-02, -7.6992e-02,  4.7776e-02,
         -4.7372e-03, -4.7319e-02, -6.4084e-02,  1.3756e-02,  6.1578e-02],
        [-3.9753e-03, -3.3341e-02, -9.0807e-02, -4.4928e-02, -3.6905e-03,
          5.1872e-02, -7.5030e-02, -3.7705e-02,  7.2325e-03,  7.8419e-03],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [-1.1178e-02, -4.0416e-02, -9.0959e-02, -2.8958e-02,  4.9563e-02,
          3.6475e-02, -4.6582e-02, -3.2763e-02,  4.0456e-02,  2.0327e-02],
        [ 4.9374e-04,  1.0592e-02, -1.0834e-01, -8.6161e-02,  4.4012e-03,
          5.8533e-02, -5.8526e-02,  8.8540e-03,  5.0868e-02,  2.7062e-02],
        [ 4.2793e-02, -5.1562e-02, -7.0316e-02, -8.6661e-02,  2.8821e-02,
          5.9291e-02, -7.5899e-03, -5.1042e-02, -1.8969e-02,  2.9452e-02],
        [-2.6757e-02, -9.6486e-02, -9.5810e-02, -5.2101e-02,  1.2008e-02,
          4.2769e-02, -4.9963e-02, -2.4326e-02, -3.5636e-02,  1.8129e-02],
        [ 2.4353e-02, -1.7624e-02, -9.6645e-02, -6.4769e-02,  3.3068e-02,
          4.1625e-02, -3.9070e-02, -2.3966e-02, -4.4149e-03,  2.9720e-02],
        [ 1.1612e-02, -4.9405e-02, -7.0849e-02, -3.7492e-02,  3.2517e-02,
          6.9406e-02, -7.1818e-02, -4.6350e-02, -3.9943e-02,  2.1958e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 4.7729e-02,  4.6565e-03, -7.2186e-02, -7.5132e-02,  3.7200e-02,
          4.7864e-02, -1.6563e-02, -1.1979e-02,  8.6644e-04,  4.6806e-02],
        [ 1.1755e-03, -4.9623e-02, -1.0965e-01, -7.3069e-02, -1.0762e-03,
          2.8836e-02,  1.1540e-03, -3.0960e-02,  1.8375e-02,  3.6794e-02],
        [ 1.5920e-02, -3.6161e-02, -7.8173e-02, -5.3683e-02,  5.0854e-02,
          6.1498e-02, -9.4034e-02, -3.3560e-02, -2.8307e-03,  1.9919e-02],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 6.4549e-02, -7.4260e-02, -6.1518e-02, -6.3584e-02,  6.0834e-02,
          3.6721e-02, -3.2211e-02, -7.9127e-02,  3.4443e-02,  6.0064e-02],
        [ 7.4437e-03, -2.0984e-02, -1.0167e-01, -5.3580e-02,  2.7302e-02,
          6.2117e-02, -7.0678e-02, -2.0438e-02,  1.8828e-02,  2.2841e-02],
        [ 9.8936e-03, -7.6981e-02, -6.1310e-02, -2.9750e-02,  3.6546e-02,
          3.4909e-02, -5.5577e-02, -3.5362e-02, -4.2334e-02,  5.2993e-02],
        [ 6.7854e-04, -5.8810e-02, -7.0261e-02, -9.2575e-02,  3.5824e-02,
          3.8963e-02, -1.4449e-02, -3.2278e-02,  5.1556e-03,  3.5941e-02],
        [ 4.2797e-02, -2.1380e-02, -9.4042e-02, -6.5469e-02,  1.3970e-02,
          4.0768e-02, -4.0473e-02, -1.8065e-02, -1.7199e-03,  3.3296e-02],
        [-6.6224e-03, -1.3180e-02, -6.5494e-02, -6.5850e-02,  9.9112e-03,
          9.0910e-02, -5.4298e-02, -6.5534e-03,  2.6744e-03,  4.3763e-02],
        [ 1.5555e-03, -3.5281e-02, -7.2224e-02, -8.4545e-02,  6.7220e-03,
          4.5304e-02, -5.7760e-02, -1.2188e-02, -2.9475e-02,  1.6610e-02],
        [ 1.9890e-02, -3.4235e-02, -9.5454e-02, -7.2502e-02,  1.7934e-02,
          6.4805e-02, -5.4793e-02, -1.9749e-02, -3.6248e-02, -2.0136e-03],
        [ 2.8964e-02, -1.7840e-02, -6.4336e-02, -6.2594e-02,  4.7344e-02,
          3.8347e-02, -1.7235e-02, -4.7374e-02, -1.3653e-02,  1.3487e-02],
        [ 8.9843e-03, -7.4622e-02, -7.7330e-02, -6.2117e-02,  4.1764e-02,
          3.9837e-02, -5.3195e-02, -2.9001e-02,  1.9979e-02,  4.2635e-02],
        [-1.1281e-02, -5.5235e-02, -7.4960e-02, -8.8369e-02,  1.1725e-02,
          9.0346e-02, -1.9225e-02, -9.1089e-03, -2.6951e-02,  5.0562e-02],
        [ 1.0422e-02, -2.8694e-02, -1.3384e-01, -6.2680e-02, -4.0576e-03,
          4.2460e-02, -2.8410e-02, -3.6864e-02, -2.4423e-02,  9.6237e-03],
        [-8.0622e-03, -6.0198e-02, -1.2272e-01, -9.4218e-02, -2.0327e-02,
          5.6627e-02, -1.7947e-02, -4.3294e-02, -4.5088e-02,  4.9459e-02],
        [ 3.1465e-02, -6.3475e-02, -1.3706e-01, -2.7501e-02,  7.4692e-03,
          4.4910e-02, -2.3597e-02, -7.2583e-02,  4.4441e-02,  4.8480e-02],
        [ 9.5033e-03, -6.7546e-02, -1.1278e-01, -7.1805e-02,  1.9694e-02,
          5.2446e-02, -4.6915e-02, -3.5823e-02, -2.6672e-02,  1.2705e-02],
        [-1.7664e-03, -3.0356e-02, -5.2647e-02, -5.0923e-02,  9.9229e-03,
          4.9579e-02, -4.8198e-02, -2.2725e-02, -1.0950e-02,  7.5908e-02],
        [ 3.5853e-02, -5.1923e-02, -5.7134e-02, -9.6459e-02,  8.7225e-03,
          1.0208e-01, -2.4951e-03, -3.2193e-02, -1.4123e-02,  3.7532e-02],
        [ 1.2903e-02, -2.9470e-02, -1.0240e-01, -6.5606e-02, -1.3130e-02,
          2.7493e-02, -6.6286e-02, -1.7580e-02, -1.0268e-02, -7.5387e-04],
        [ 3.9866e-02, -6.3604e-02, -9.2424e-02, -1.8145e-02,  6.2771e-03,
          5.3942e-02, -2.5060e-03, -3.9736e-02, -2.9609e-02,  4.5879e-02],
        [ 1.5286e-02, -5.6043e-02, -9.8077e-02, -1.0474e-01,  2.1392e-02,
          4.1300e-02, -9.3875e-03, -3.6225e-02, -1.0468e-03,  2.3071e-02],
        [ 5.3243e-02, -1.7418e-02, -6.7548e-02, -3.0669e-02,  2.9064e-02,
          8.0370e-02, -6.2385e-02, -7.6080e-02,  1.9496e-02,  4.8845e-02],
        [-7.7391e-03, -4.2591e-02, -1.2304e-01, -9.9758e-02,  3.0167e-02,
          1.2025e-02, -2.4776e-02, -7.8643e-02,  1.0363e-02,  1.3628e-02],
        [ 4.0937e-02, -4.6699e-02, -8.6771e-02, -8.1032e-02,  6.7437e-02,
          4.7374e-02, -4.3250e-03, -6.2832e-02,  3.1531e-04,  6.3860e-02],
        [ 7.6590e-03, -2.7796e-02, -7.2230e-02, -6.5671e-02, -2.4608e-03,
          7.4804e-02, -3.8319e-02, -1.9051e-02, -1.1377e-03,  3.0315e-02],
        [-7.3344e-03, -3.1084e-02, -6.7896e-02, -6.4647e-02, -5.7906e-03,
          4.1201e-02, -3.3180e-02, -3.3913e-02, -3.5557e-03,  2.4866e-02]],
       device='mps:0', grad_fn=<LinearBackward0>) tensor([7, 0, 3, 0, 0, 1, 7, 9, 2, 2, 0, 0, 9, 0, 3, 9, 7, 9, 1, 7, 9, 7, 1, 0,
        0, 0, 3, 9, 8, 0, 9, 8, 2, 6, 0, 3, 9, 1, 3, 0, 3, 0, 3, 0, 3, 6, 7, 7,
        0, 9, 9, 9, 9, 6, 7, 0, 8, 3, 1, 1, 7, 9, 8, 2], device='mps:0') tensor(2.3044, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[ 1.5001e-02, -2.4317e-02, -9.6092e-02, -4.5386e-02,  1.9142e-02,
          1.7589e-02, -3.6411e-02, -1.7132e-02, -1.0519e-02,  3.2400e-02],
        [ 4.8513e-02, -2.0605e-02, -8.7895e-02, -3.8833e-02,  2.6858e-02,
          3.3785e-02, -4.6490e-02,  6.4483e-03, -1.6461e-02,  6.7407e-02],
        [ 5.2186e-02, -8.6194e-02, -7.7324e-02, -6.1783e-02,  2.2660e-02,
          3.2762e-02, -4.7886e-02, -6.8551e-02, -3.2778e-02,  3.2820e-02],
        [ 2.3043e-02, -6.3882e-02, -7.9173e-02, -3.4427e-02, -5.7518e-04,
          3.3389e-02, -4.5834e-02, -2.1257e-02, -3.1910e-02,  3.1667e-02],
        [ 3.8078e-02, -4.5804e-02, -1.0045e-01, -6.9722e-02,  1.9309e-03,
          2.8644e-02, -2.8495e-02, -5.4856e-02, -1.5686e-02,  5.9758e-02],
        [ 2.3375e-02, -3.4161e-02, -8.1081e-02, -9.0999e-02, -1.1078e-02,
          6.4335e-02, -2.5180e-02, -4.2009e-02, -1.4219e-02,  4.7543e-02],
        [ 4.9916e-02, -5.5589e-02, -7.4972e-02, -4.3805e-02,  1.3444e-02,
          7.4269e-02, -5.4626e-02, -2.3150e-02, -1.2007e-02,  1.3286e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [-6.4825e-03, -4.0872e-02, -7.3447e-02, -1.1937e-02,  3.0927e-03,
          8.1310e-02,  3.1084e-03, -3.5430e-02, -1.6151e-02,  6.1291e-02],
        [-7.0495e-03, -1.7228e-02, -1.2277e-01, -9.4239e-02,  5.9890e-03,
         -6.1808e-03, -3.1208e-02, -7.5787e-02, -1.4895e-02,  1.3742e-02],
        [ 6.1543e-02, -7.2656e-02, -9.6704e-02, -8.3352e-02,  1.6915e-02,
          6.2911e-02, -1.8400e-02, -8.5107e-02, -3.9816e-02,  7.4679e-03],
        [-1.1408e-02, -4.6092e-02, -9.7627e-02, -1.2756e-02,  1.7089e-02,
         -3.0917e-03, -7.7207e-02, -2.0344e-02,  1.7261e-02,  2.9339e-02],
        [ 4.2729e-02, -4.0029e-02, -8.5290e-02, -9.1784e-02,  1.1776e-02,
          7.1396e-02, -2.6482e-02, -1.4159e-02, -2.7569e-02,  1.5441e-02],
        [ 4.2519e-03, -4.4440e-02, -7.7422e-02, -7.3050e-02,  2.8981e-02,
          7.0774e-02, -4.1154e-02, -1.1229e-02, -5.5738e-03,  2.0349e-02],
        [ 4.9137e-02, -3.4235e-02, -1.2025e-01, -7.9170e-02,  2.9366e-02,
          5.9278e-02, -9.3778e-03, -1.7983e-02, -2.0472e-02,  1.6061e-02],
        [ 4.0011e-04, -6.1130e-02, -6.7522e-02, -3.6550e-02,  2.1807e-02,
          4.0619e-02, -5.2709e-02, -6.1001e-02, -1.4555e-04,  3.9479e-02],
        [ 6.7079e-02,  8.3963e-04, -7.4894e-02, -2.3669e-02,  3.9264e-02,
          4.6286e-02, -6.2341e-02, -5.4688e-03,  2.9301e-02,  4.2407e-02],
        [ 1.0572e-02, -6.4207e-03, -9.2976e-02, -7.9015e-02, -2.6992e-02,
          9.4678e-02, -4.6331e-02, -6.6075e-03,  7.7661e-03,  4.5618e-02],
        [ 1.2208e-02, -4.2443e-02, -8.9354e-02, -4.2341e-02, -1.5570e-02,
          3.3271e-02, -4.0502e-02, -6.1946e-02, -1.7031e-02,  3.4724e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [-7.7529e-03, -7.2789e-02, -7.5776e-02, -8.4303e-02,  1.2266e-02,
          4.9328e-02, -5.5146e-02, -8.6516e-02, -6.2035e-02,  1.5078e-02],
        [ 3.0387e-03, -4.1034e-02, -1.2861e-01, -6.1578e-02, -2.1708e-02,
          2.6927e-02, -7.0021e-02, -3.1372e-02, -3.3448e-02,  1.4466e-02],
        [ 3.5859e-03, -3.7401e-02, -1.1349e-01, -7.9695e-02, -4.6774e-03,
         -7.7311e-03, -4.2785e-02, -2.0389e-02,  2.8483e-03,  1.8359e-02],
        [ 3.5078e-02, -4.7457e-02, -6.3950e-02, -4.5136e-02, -2.4122e-02,
          3.6731e-02, -7.1223e-02, -1.6768e-02, -6.2828e-02,  4.0240e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [-1.8100e-02, -4.3834e-02, -1.1766e-01, -9.1237e-02,  8.8249e-03,
          2.0316e-02, -5.1502e-02, -6.6793e-02,  3.1687e-03,  2.1584e-02],
        [ 2.2414e-02, -2.4005e-02, -1.1127e-01, -8.2310e-02,  7.7715e-03,
         -9.4877e-03, -8.3230e-02, -2.6728e-02, -2.0929e-02,  3.1895e-02],
        [ 2.7106e-02, -4.7401e-02, -1.0122e-01, -3.6867e-02,  4.1410e-02,
          9.0777e-03, -5.3287e-02, -7.3265e-02, -3.7715e-03,  6.1969e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [ 4.8967e-02, -2.3151e-02, -5.7249e-02, -6.9619e-02,  2.4692e-02,
          7.9485e-02, -6.6652e-03, -2.9402e-02,  4.9980e-03,  5.1423e-02],
        [ 3.4268e-02, -2.1509e-02, -1.3156e-01, -6.5191e-02, -5.7296e-04,
          1.0720e-02, -5.1639e-02, -6.6921e-02,  1.8662e-02,  4.6306e-02],
        [ 3.8994e-02, -3.2930e-02, -9.8752e-02, -4.5913e-02,  4.7603e-03,
          2.9648e-02, -3.6349e-02, -2.5994e-02, -6.4857e-03,  4.7311e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [ 2.0330e-02, -5.2561e-02, -8.9249e-02, -2.2645e-02,  2.9858e-02,
          1.7975e-02, -7.4204e-02, -4.1125e-02, -1.5920e-02,  5.5087e-02],
        [-1.0032e-03, -5.0487e-02, -8.8435e-02, -3.9912e-02,  1.1110e-02,
          3.1577e-02, -6.8152e-02, -3.8026e-02,  2.2377e-02,  2.5580e-02],
        [ 1.7618e-02, -5.9612e-02, -7.7119e-02, -4.5507e-02,  3.4616e-02,
          5.1471e-02, -3.3984e-02, -2.4927e-02,  1.2600e-02,  4.0817e-02],
        [ 3.8135e-02, -6.7353e-02, -1.0960e-01, -6.1218e-02,  4.1835e-02,
          3.3668e-02, -5.8354e-02, -5.5431e-02, -1.8367e-02,  1.4238e-02],
        [ 6.7734e-03, -4.7623e-02, -1.1009e-01, -1.0529e-01, -7.6091e-03,
          6.6365e-02, -1.0222e-02, -4.7322e-02, -1.6018e-02,  7.0989e-02],
        [ 9.6819e-03, -2.4306e-02, -9.4140e-02, -5.6658e-02,  2.3567e-02,
          5.6732e-02, -4.6957e-02, -1.5594e-02,  3.7803e-03,  1.3246e-02],
        [ 4.0696e-02, -2.2396e-02, -8.9451e-02, -5.4290e-02,  9.6839e-03,
          3.9006e-02, -5.9488e-02, -2.0909e-02, -1.8616e-02,  6.6974e-02],
        [ 5.9641e-02, -4.9080e-02, -7.4932e-02, -6.6140e-02,  2.7822e-02,
          4.8863e-02, -3.4778e-02, -5.1976e-02, -1.6638e-02,  8.3342e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [ 1.5141e-02, -5.0674e-02, -9.9043e-02, -8.3563e-02, -3.1072e-03,
          5.8528e-02, -5.4653e-02, -6.7914e-02, -3.1432e-02,  3.5788e-02],
        [-2.3794e-02, -4.1720e-02, -1.0646e-01, -2.9800e-02,  5.3790e-03,
          2.2550e-02, -6.3161e-02, -5.7906e-02,  9.3223e-03,  1.3482e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [ 4.0196e-02, -4.7165e-02, -6.6599e-02, -1.4496e-02,  1.1127e-02,
          2.9633e-02, -3.9643e-02, -2.3870e-02,  3.2870e-02,  5.1642e-02],
        [ 2.1977e-02, -8.0384e-02, -5.8199e-02, -2.6157e-02,  8.4272e-03,
          4.6193e-02, -6.5143e-02, -5.7406e-02, -2.9726e-02,  2.6345e-02],
        [ 2.2945e-02, -2.1973e-02, -8.5682e-02, -4.8616e-02,  4.9678e-02,
          3.1516e-02, -6.4259e-02, -2.5394e-02,  1.3428e-02,  8.0700e-02],
        [-1.7296e-02,  1.0009e-02, -1.4026e-01, -4.9267e-02,  8.3487e-03,
          6.8221e-02, -7.1650e-02, -1.7769e-03,  5.4122e-04,  5.3151e-02],
        [ 1.7603e-02, -3.4423e-02, -1.1644e-01, -7.1071e-02, -3.4449e-03,
          2.5583e-02, -5.2670e-02, -3.5955e-02, -4.6838e-03,  5.9021e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [ 2.9964e-02, -1.8840e-02, -6.5336e-02, -6.1594e-02,  4.6344e-02,
          3.7347e-02, -1.8235e-02, -4.6374e-02, -1.4653e-02,  1.4487e-02],
        [ 8.3984e-03, -4.0538e-02, -8.3730e-02, -4.7694e-02, -1.8875e-02,
          3.5450e-02, -2.0422e-02, -2.8532e-02, -1.8078e-02,  5.9585e-02],
        [-1.3275e-05, -1.2235e-02, -1.2229e-01, -7.7745e-02,  6.7700e-04,
          3.5021e-02, -5.2531e-02, -4.5727e-02, -1.6554e-02,  2.1448e-02],
        [-9.5053e-03, -2.9786e-02, -1.5308e-01, -9.3253e-02, -1.8996e-02,
          7.6748e-03, -6.0652e-02, -6.0459e-02, -4.1953e-02,  4.0805e-02],
        [-1.4054e-02, -5.5736e-02, -8.4056e-02, -5.2541e-02,  3.3283e-02,
          1.4747e-02, -4.6095e-02, -4.7188e-02,  6.8966e-03,  5.4875e-02],
        [ 5.6255e-02, -1.8226e-02, -5.8632e-02, -6.1294e-02, -2.2670e-02,
          7.0059e-02, -5.0711e-02, -3.3941e-02, -2.1634e-02,  5.1227e-02],
        [ 3.4847e-02, -7.4094e-02, -7.6628e-02, -8.8514e-02,  1.6766e-03,
          2.0442e-02, -8.5830e-02, -3.9074e-02, -4.2290e-02,  4.3589e-03],
        [ 2.5032e-02, -5.4150e-02, -1.1470e-01, -6.1506e-02,  1.7884e-02,
          5.4857e-02, -5.9408e-03, -3.2635e-02,  1.6639e-04,  2.4065e-02],
        [ 1.6892e-02, -4.4749e-02, -6.6931e-02, -4.2049e-02,  5.4564e-02,
          1.3419e-02, -7.2136e-02, -4.8882e-02,  8.0887e-03,  7.2724e-02],
        [ 1.5399e-02, -3.5377e-02, -9.2486e-02, -6.4833e-02,  1.3405e-02,
          3.5822e-02, -3.9006e-02, -3.3246e-02, -1.5363e-02,  1.3313e-02],
        [ 1.9491e-02, -1.8396e-02, -1.0564e-01, -6.9200e-02,  2.8589e-02,
          5.6427e-02, -5.4814e-02, -3.7809e-02, -2.8670e-02,  2.6691e-02],
        [-2.3568e-02, -2.8360e-02, -7.2783e-02, -8.5011e-02, -1.2453e-02,
          6.8572e-02, -4.4747e-02, -1.5212e-02, -4.6257e-02,  2.6809e-02]],
       device='mps:0', grad_fn=<LinearBackward0>) tensor([9, 1, 1, 0, 3, 7, 0, 9, 2, 0, 9, 2, 6, 1, 1, 9, 0, 9, 3, 0, 1, 1, 3, 2,
        0, 2, 1, 2, 7, 0, 7, 2, 8, 9, 0, 7, 8, 9, 9, 0, 9, 6, 0, 0, 7, 1, 0, 9,
        7, 0, 0, 1, 9, 8, 0, 1, 0, 8, 3, 3, 3, 3, 1, 2], device='mps:0') tensor(2.3050, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0338, -0.0164, -0.1206, -0.0941, -0.0028, -0.0067, -0.0426, -0.0604,
         -0.0206,  0.0255],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0270, -0.0172, -0.0649, -0.0599,  0.0460,  0.0365, -0.0200, -0.0451,
         -0.0155,  0.0163],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0270, -0.0172, -0.0649, -0.0599,  0.0460,  0.0365, -0.0200, -0.0451,
         -0.0155,  0.0163],
        [ 0.0310, -0.0181, -0.0656, -0.0607,  0.0453,  0.0363, -0.0192, -0.0458,
         -0.0157,  0.0155],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0292, -0.0519, -0.1020, -0.0558,  0.0466,  0.0306, -0.0090, -0.0524,
         -0.0056,  0.0388],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0309, -0.0174, -0.0653, -0.0610,  0.0455,  0.0373, -0.0180, -0.0462,
         -0.0212,  0.0161],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0204, -0.0620, -0.1113, -0.0750,  0.0257,  0.0317, -0.0087, -0.0483,
         -0.0060,  0.0515],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0310, -0.0181, -0.0656, -0.0607,  0.0453,  0.0363, -0.0192, -0.0458,
         -0.0157,  0.0155],
        [ 0.0401, -0.0569, -0.0832, -0.0482,  0.0094,  0.0432, -0.0397, -0.0549,
          0.0082,  0.0320],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0182, -0.0031, -0.1127, -0.0899, -0.0089,  0.0386, -0.0418, -0.0245,
          0.0005,  0.0281],
        [ 0.0384, -0.0561, -0.1253, -0.0690,  0.0080, -0.0192, -0.0211, -0.0716,
          0.0273,  0.0238],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0318, -0.0182, -0.0653, -0.0603,  0.0460,  0.0367, -0.0189, -0.0450,
         -0.0148,  0.0113],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0310, -0.0181, -0.0656, -0.0607,  0.0453,  0.0363, -0.0192, -0.0458,
         -0.0157,  0.0155],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0310, -0.0181, -0.0656, -0.0607,  0.0453,  0.0363, -0.0192, -0.0458,
         -0.0157,  0.0155],
        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
             nan,     nan],
        [ 0.0310, -0.0181, -0.0656, -0.0607,  0.0453,  0.0363, -0.0192, -0.0458,
         -0.0157,  0.0155],
        [ 0.0294, -0.0354, -0.1358, -0.0834,  0.0115, -0.0153, -0.0371, -0.0561,
          0.0005,  0.0209]], device='mps:0', grad_fn=<LinearBackward0>) tensor([7, 1, 0, 1, 2, 1, 9, 9, 3, 3, 7, 2, 9, 1, 9, 0, 8, 9, 8, 9, 0, 1, 9, 0,
        9, 7, 7, 9, 9, 9, 3, 3, 9, 7, 9, 6, 2, 2, 9, 6, 9, 3, 9, 6, 9, 0, 0, 1,
        2, 3, 0, 1, 9, 0, 3, 2, 2, 0, 8, 0, 7, 9, 9, 0], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([6, 6, 0, 3, 9, 2, 0, 9, 3, 7, 9, 0, 1, 0, 8, 3, 9, 6, 7, 0, 0, 7, 8, 9,
        2, 8, 8, 7, 3, 3, 7, 8, 8, 1, 1, 7, 9, 2, 8, 8, 7, 0, 9, 7, 3, 2, 2, 0,
        6, 7, 9, 2, 0, 0, 2, 9, 7, 0, 0, 9, 2, 0, 9, 3], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 9, 6, 2, 0, 2, 0, 6, 6, 9, 0, 0, 6, 0, 0, 9, 2, 8, 9,
        7, 9, 0, 0, 9, 8, 0, 6, 6, 9, 1, 6, 0, 2, 0, 7, 1, 7, 6, 1, 7, 3, 8, 1,
        1, 8, 0, 0, 9, 1, 2, 8, 0, 3, 8, 9, 8, 0, 2, 6], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([9, 2, 9, 8, 7, 9, 2, 7, 9, 0, 1, 3, 9, 6, 3, 1, 0, 0, 7, 9, 8, 8, 8, 8,
        0, 7, 8, 3, 0, 6, 9, 1, 3, 2, 6, 2, 7, 7, 3, 0, 8, 6, 2, 2, 9, 0, 1, 1,
        7, 9, 0, 7, 3, 9, 9, 2, 2, 9, 6, 9, 7, 9, 0, 1], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([3, 8, 0, 0, 2, 9, 9, 0, 2, 8, 9, 9, 2, 8, 0, 6, 0, 6, 2, 0, 3, 9, 9, 8,
        0, 1, 7, 8, 0, 1, 7, 8, 6, 2, 3, 7, 9, 0, 2, 8, 1, 1, 7, 3, 9, 2, 7, 0,
        8, 2, 9, 0, 3, 3, 0, 8, 9, 2, 6, 6, 0, 8, 1, 7], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([0, 9, 9, 7, 3, 9, 3, 0, 0, 1, 7, 0, 7, 1, 2, 2, 8, 3, 6, 3, 9, 1, 8, 6,
        3, 9, 6, 8, 0, 8, 2, 2, 2, 0, 0, 1, 7, 0, 2, 7, 9, 7, 3, 8, 1, 3, 0, 9,
        6, 3, 7, 0, 9, 2, 6, 9, 8, 6, 9, 6, 9, 3, 0, 7], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([9, 8, 9, 1, 7, 2, 9, 6, 1, 1, 2, 6, 9, 8, 0, 2, 9, 9, 1, 0, 1, 3, 8, 6,
        6, 0, 1, 6, 8, 8, 0, 9, 9, 7, 9, 2, 7, 3, 6, 0, 0, 9, 1, 9, 8, 3, 7, 6,
        1, 3, 6, 7, 2, 0, 0, 3, 6, 7, 3, 1, 7, 1, 3, 0], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([9, 0, 0, 3, 2, 9, 0, 6, 6, 7, 9, 9, 9, 7, 9, 2, 8, 0, 6, 0, 0, 9, 6, 8,
        3, 2, 2, 0, 7, 1, 3, 1, 7, 7, 9, 9, 8, 0, 9, 8, 0, 0, 0, 0, 1, 9, 1, 2,
        6, 9, 8, 9, 1, 6, 0, 8, 0, 2, 8, 0, 9, 7, 9, 6], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([8, 0, 9, 9, 7, 8, 0, 6, 0, 2, 2, 0, 8, 8, 0, 0, 3, 6, 1, 0, 0, 6, 7, 1,
        3, 0, 8, 6, 8, 7, 9, 6, 1, 0, 7, 9, 9, 6, 6, 0, 7, 6, 0, 1, 0, 3, 0, 8,
        8, 2, 1, 1, 3, 3, 6, 7, 6, 8, 1, 3, 0, 7, 2, 3], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 9, 6, 2, 0, 2, 0, 6, 6, 9, 0, 0, 6, 0, 0, 9, 2, 8, 9,
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([9, 1, 6, 3, 9, 8, 7, 1, 9, 2, 3, 9, 0, 3, 8, 1, 7, 0, 0, 6, 9, 7, 6, 7,
        1, 2, 9, 2, 7, 0, 8, 9, 6, 9, 6, 3, 0, 3, 0, 0, 7, 0, 1, 0, 2, 7, 9, 9,
        3, 0, 0, 9, 3, 2, 3, 2, 7, 1, 0, 9, 9, 0, 2, 7], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([0, 8, 9, 3, 9, 3, 9, 1, 9, 6, 7, 6, 8, 1, 2, 9, 0, 9, 7, 9, 6, 8, 6, 7,
        2, 7, 8, 6, 0, 3, 0, 7, 0, 7, 7, 9, 1, 7, 7, 3, 8, 2, 0, 6, 3, 0, 9, 7,
        1, 8, 7, 0, 9, 0, 1, 0, 8, 8, 3, 1, 3, 7, 3, 2], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([9, 0, 6, 6, 0, 7, 9, 0, 0, 9, 8, 9, 9, 9, 9, 8, 1, 8, 9, 7, 0, 8, 1, 0,
        8, 0, 0, 2, 2, 2, 1, 2, 0, 9, 3, 8, 2, 7, 0, 6, 3, 1, 8, 8, 0, 9, 3, 7,
        6, 9, 7, 0, 8, 0, 9, 0, 9, 7, 0, 9, 9, 1, 8, 8], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([2, 0, 3, 8, 6, 9, 9, 1, 2, 7, 6, 0, 8, 0, 2, 9, 9, 7, 0, 0, 8, 9, 7, 1,
        6, 9, 6, 1, 3, 9, 2, 3, 6, 1, 0, 3, 9, 0, 7, 9, 3, 0, 2, 7, 6, 8, 9, 7,
        6, 2, 3, 1, 3, 7, 9, 0, 9, 6, 9, 6, 6, 0, 8, 9], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([2, 9, 9, 8, 1, 7, 9, 1, 0, 8, 9, 9, 7, 9, 2, 7, 6, 1, 6, 0, 3, 7, 9, 2,
        2, 7, 0, 2, 3, 3, 0, 0, 3, 3, 7, 0, 7, 2, 1, 0, 3, 8, 7, 1, 3, 9, 1, 8,
        9, 0, 3, 2, 9, 9, 6, 0, 7, 9, 6, 9, 7, 9, 0, 2], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([8, 0, 0, 7, 1, 1, 7, 6, 7, 1, 6, 2, 9, 7, 9, 8, 6, 0, 0, 1, 0, 0, 0, 9,
        7, 3, 8, 3, 9, 7, 6, 1, 2, 7, 0, 3, 1, 1, 0, 7, 0, 3, 9, 8, 3, 0, 7, 3,
        3, 0, 7, 9, 1, 6, 3, 7, 1, 0, 0, 0, 3, 3, 9, 0], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='mps:0',
       grad_fn=<LinearBackward0>) tensor([6, 6, 3, 3, 6, 0, 0, 9, 6, 0, 9, 1, 9, 7, 3, 7, 1, 0, 3, 9, 2, 0, 7, 6,
        0, 3, 1, 0, 1, 7, 0, 8, 6, 6, 0, 9, 7, 1, 9, 9, 3, 3, 7, 0, 8, 0, 9, 1,
        9, 2, 3, 8, 0, 0, 6, 0, 7, 8, 6, 0, 0, 2, 0, 7], device='mps:0') tensor(nan, device='mps:0', grad_fn=<NllLossBackward0>)